#!/usr/bin/env python3
"""
AMSR2 Sequential Trainer - Memory Protected Edition
–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª –∑–∞ —Ñ–∞–π–ª–æ–º, –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö

–ê–≤—Ç–æ—Ä: Volodymyr Didur
–í–µ—Ä—Å–∏—è: 4.0 - Sequential Processing Edition
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import glob
from typing import Tuple, List, Optional, Dict
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import logging
import time
import json
import argparse
from pathlib import Path
import psutil
import threading
import signal
import sys
from tqdm import tqdm
import gc
import warnings

warnings.filterwarnings('ignore', category=UserWarning)

# ====== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï ======
EMERGENCY_STOP = False
MEMORY_THRESHOLD = 80.0
MONITOR_INTERVAL = 6

# ====== –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ======
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('amsr2_sequential.log')
    ]
)
logger = logging.getLogger(__name__)


# ====== –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò ======
class MemoryMonitor:
    """–£—Å–∏–ª–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""

    def __init__(self, threshold=70.0):  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–æ 70%
        self.threshold = threshold
        self.monitoring = False
        self.monitor_thread = None
        self.check_count = 0
        self.critical_warnings = 0

    def check_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞–º—è—Ç–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()

        return {
            'percent': memory.percent,
            'available_gb': memory.available / 1024 ** 3,
            'total_gb': memory.total / 1024 ** 3,
            'used_gb': memory.used / 1024 ** 3,
            'swap_percent': swap.percent,
            'swap_used_gb': swap.used / 1024 ** 3
        }

    def force_cleanup(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ —Å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        logger.info("üßπ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        for i in range(5):
            gc.collect()
            time.sleep(0.5)

        # –ü–æ–ø—ã—Ç–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é –ø–∞–º—è—Ç—å
        try:
            import ctypes
            libc = ctypes.CDLL("libc.so.6")
            libc.malloc_trim(0)
        except:
            pass

        logger.info("   –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def emergency_stop_if_needed(self):
        """–£—Å–∏–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏"""
        memory_info = self.check_memory()
        self.check_count += 1

        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è
        critical_conditions = []

        if memory_info['percent'] > self.threshold:
            critical_conditions.append(f"RAM: {memory_info['percent']:.1f}%")

        if memory_info['available_gb'] < 2.0:  # –ú–∏–Ω–∏–º—É–º 2GB
            critical_conditions.append(f"–î–æ—Å—Ç—É–ø–Ω–æ: {memory_info['available_gb']:.1f}GB")

        if memory_info['swap_percent'] > 50:  # Swap –Ω–µ –¥–æ–ª–∂–µ–Ω –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è
            critical_conditions.append(f"Swap: {memory_info['swap_percent']:.1f}%")

        # CPU –ø—Ä–æ–≤–µ—Ä–∫–∞
        cpu_percent = psutil.cpu_percent(interval=1)
        if cpu_percent > CPU_THRESHOLD:
            critical_conditions.append(f"CPU: {cpu_percent:.1f}%")

        if critical_conditions:
            self.critical_warnings += 1
            logger.critical(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –°–û–°–¢–û–Ø–ù–ò–ï #{self.critical_warnings}: {', '.join(critical_conditions)}")

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            self.force_cleanup()

            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
            memory_info = self.check_memory()
            if memory_info['percent'] > self.threshold:
                global EMERGENCY_STOP
                EMERGENCY_STOP = True
                raise MemoryError(
                    f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_info['percent']:.1f}% (–ø–æ—Ä–æ–≥: {self.threshold}%)")

        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–∏ –∫ –ª–∏–º–∏—Ç–∞–º
        if memory_info['percent'] > self.threshold - 10:  # 60% –¥–ª—è –ø–æ—Ä–æ–≥–∞ 70%
            logger.warning(f"‚ö†Ô∏è –ü—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –∫ –ª–∏–º–∏—Ç—É –ø–∞–º—è—Ç–∏: {memory_info['percent']:.1f}%")

        return memory_info

    def monitor_loop(self):
        """–¶–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        consecutive_warnings = 0

        while self.monitoring:
            try:
                memory_info = self.check_memory()
                cpu_percent = psutil.cpu_percent(interval=1)

                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 –ø—Ä–æ–≤–µ—Ä–æ–∫
                if self.check_count % 10 == 0:
                    logger.info(f"üìä –°–∏—Å—Ç–µ–º–∞ - RAM: {memory_info['percent']:.1f}%, "
                                f"CPU: {cpu_percent:.1f}%, "
                                f"–î–æ—Å—Ç—É–ø–Ω–æ: {memory_info['available_gb']:.1f}GB, "
                                f"Swap: {memory_info['swap_percent']:.1f}%")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π
                warning_conditions = []

                if memory_info['percent'] > self.threshold - 5:  # 65% –¥–ª—è –ø–æ—Ä–æ–≥–∞ 70%
                    warning_conditions.append(f"RAM –±–ª–∏–∑–∫–æ –∫ –ª–∏–º–∏—Ç—É: {memory_info['percent']:.1f}%")

                if cpu_percent > 85:
                    warning_conditions.append(f"–í—ã—Å–æ–∫–∏–π CPU: {cpu_percent:.1f}%")

                if memory_info['swap_percent'] > 25:
                    warning_conditions.append(f"–ê–∫—Ç–∏–≤–Ω—ã–π Swap: {memory_info['swap_percent']:.1f}%")

                if warning_conditions:
                    consecutive_warnings += 1
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ #{consecutive_warnings}: {', '.join(warning_conditions)}")

                    if consecutive_warnings >= 5:  # 5 –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –ø–æ–¥—Ä—è–¥
                        logger.error("üö® –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –ø–æ–¥—Ä—è–¥ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞")
                        self.force_cleanup()
                        consecutive_warnings = 0
                else:
                    consecutive_warnings = 0

                time.sleep(self.monitor_interval)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
                time.sleep(self.monitor_interval)

    def start_monitoring(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        if not self.monitoring:
            logger.info(f"üîç –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏ (–ø–æ—Ä–æ–≥: {self.threshold}%)")
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self.monitor_loop, daemon=True)
            self.monitor_thread.start()

    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)


memory_monitor = MemoryMonitor(MEMORY_THRESHOLD)


# ====== DATASET –î–õ–Ø –û–î–ù–û–ì–û –§–ê–ô–õ–ê ======
class SingleFileAMSR2Dataset(Dataset):
    """Dataset –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ NPZ —Ñ–∞–π–ª–∞"""

    def __init__(self, npz_path: str, preprocessor,
                 degradation_scale: int = 4, augment: bool = True,
                 filter_orbit_type: Optional[str] = None):

        self.npz_path = npz_path
        self.preprocessor = preprocessor
        self.degradation_scale = degradation_scale
        self.augment = augment
        self.filter_orbit_type = filter_orbit_type

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
        self.swaths = self._load_file_data()

    def _load_file_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(self.npz_path)}")

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            memory_monitor.emergency_stop_if_needed()

            with np.load(self.npz_path, allow_pickle=True) as data:
                if 'swath_array' not in data:
                    logger.error(f"‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–∞: {self.npz_path}")
                    return []

                swath_array = data['swath_array']
                valid_swaths = []

                for swath_idx, swath_dict in enumerate(swath_array):
                    swath = swath_dict.item() if isinstance(swath_dict, np.ndarray) else swath_dict

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã swath
                    if 'temperature' not in swath or 'metadata' not in swath:
                        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ swath {swath_idx}: –Ω–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
                        continue

                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –æ—Ä–±–∏—Ç—ã
                    if self.filter_orbit_type is not None:
                        orbit_type = swath['metadata'].get('orbit_type', 'U')
                        if orbit_type != self.filter_orbit_type:
                            continue

                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
                    raw_temperature = swath['temperature']
                    metadata = swath['metadata']

                    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ scale factor
                    scale_factor = metadata.get('scale_factor', 1.0)

                    if raw_temperature.dtype != np.float32:
                        temperature = raw_temperature.astype(np.float32) * scale_factor
                    else:
                        temperature = raw_temperature * scale_factor

                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    temperature = np.where(temperature < 50, np.nan, temperature)
                    temperature = np.where(temperature > 350, np.nan, temperature)

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
                    valid_pixels = np.sum(~np.isnan(temperature))
                    total_pixels = temperature.size

                    if valid_pixels / total_pixels < 0.1:
                        continue

                    valid_swaths.append({
                        'temperature': temperature,
                        'metadata': metadata
                    })

                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(valid_swaths)} –≤–∞–ª–∏–¥–Ω—ã—Ö swaths –∏–∑ {len(swath_array)}")
                return valid_swaths

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {self.npz_path}: {e}")
            return []

    def __len__(self):
        return len(self.swaths)

    def __getitem__(self, idx):
        if EMERGENCY_STOP or idx >= len(self.swaths):
            empty_tensor = torch.zeros(1, self.preprocessor.target_height, self.preprocessor.target_width)
            return empty_tensor, empty_tensor

        try:
            swath = self.swaths[idx]
            temperature = swath['temperature'].copy()

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            temperature = self.preprocessor.crop_and_pad_to_target(temperature)
            temperature = self.preprocessor.normalize_brightness_temperature(temperature)

            if self.augment:
                temperature = self._augment_data(temperature)

            degraded = self._create_degradation(temperature)

            high_res = torch.from_numpy(temperature).unsqueeze(0).float()
            low_res = torch.from_numpy(degraded).unsqueeze(0).float()

            return low_res, high_res

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ __getitem__[{idx}]: {e}")
            empty_tensor = torch.zeros(1, self.preprocessor.target_height, self.preprocessor.target_width)
            return empty_tensor, empty_tensor

    def _create_degradation(self, high_res: np.ndarray) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        try:
            h, w = high_res.shape
            scale = self.degradation_scale

            # –ü—Ä–æ—Å—Ç–æ–µ downsample
            down_h, down_w = h // scale, w // scale
            if down_h == 0 or down_w == 0:
                return high_res

            cropped = high_res[:down_h * scale, :down_w * scale]
            reshaped = cropped.reshape(down_h, scale, down_w, scale)
            downsampled = np.mean(reshaped, axis=(1, 3))

            # –õ–µ–≥–∫–∏–π —à—É–º
            noise = np.random.normal(0, 0.01, downsampled.shape).astype(np.float32)
            downsampled = downsampled + noise

            # Upsampling
            upsampled = np.repeat(np.repeat(downsampled, scale, axis=0), scale, axis=1)
            result = upsampled[:h, :w]

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏: {e}")
            return high_res

    def _augment_data(self, data: np.ndarray) -> np.ndarray:
        """–õ–µ–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è"""
        if not self.augment or np.random.rand() > 0.3:
            return data

        if np.random.rand() > 0.5:
            data = np.fliplr(data)
        if np.random.rand() > 0.5:
            data = np.flipud(data)

        return data


# ====== PREPROCESSOR ======
class AMSR2NPZDataPreprocessor:
    """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è AMSR2 –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏"""

    def __init__(self, target_height: int = 2000, target_width: int = 420):
        self.target_height = target_height
        self.target_width = target_width
        logger.info(f"üìè Preprocessor –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —Ä–∞–∑–º–µ—Ä: {target_height}x{target_width}")

    def crop_and_pad_to_target(self, temperature: np.ndarray) -> np.ndarray:
        """–û–±—Ä–µ–∑–∫–∞ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ target —Ä–∞–∑–º–µ—Ä–∞ AMSR2"""
        original_shape = temperature.shape
        h, w = original_shape

        # –û–±—Ä–µ–∑–∫–∞ –µ—Å–ª–∏ –±–æ–ª—å—à–µ target
        if h > self.target_height:
            start_h = (h - self.target_height) // 2
            temperature = temperature[start_h:start_h + self.target_height]

        if w > self.target_width:
            start_w = (w - self.target_width) // 2
            temperature = temperature[:, start_w:start_w + self.target_width]

        current_h, current_w = temperature.shape

        # –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –µ—Å–ª–∏ –º–µ–Ω—å—à–µ target
        if current_h < self.target_height or current_w < self.target_width:
            pad_h = max(0, self.target_height - current_h)
            pad_w = max(0, self.target_width - current_w)

            pad_top = pad_h // 2
            pad_bottom = pad_h - pad_top
            pad_left = pad_w // 2
            pad_right = pad_w - pad_left

            temperature = np.pad(temperature,
                                 ((pad_top, pad_bottom), (pad_left, pad_right)),
                                 mode='reflect')

        final_shape = temperature.shape
        if original_shape != final_shape:
            logger.debug(f"–†–∞–∑–º–µ—Ä –∏–∑–º–µ–Ω–µ–Ω: {original_shape} ‚Üí {final_shape}")

        return temperature

    def normalize_brightness_temperature(self, temperature: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è brightness temperature"""
        valid_mask = ~np.isnan(temperature)
        if np.sum(valid_mask) > 0:
            mean_temp = np.mean(temperature[valid_mask])
            temperature = np.where(np.isnan(temperature), mean_temp, temperature)
        else:
            temperature = np.full_like(temperature, 250.0)

        temperature = np.clip(temperature, 50, 350)
        normalized = (temperature - 200) / 150

        return normalized.astype(np.float32)


# ====== –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ======
class ResNetBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class UNetResNetEncoder(nn.Module):
    def __init__(self, in_channels: int = 1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, 64, 7, 2, 3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(3, 2, 1)

        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)

    def _make_layer(self, in_channels: int, out_channels: int, blocks: int, stride: int):
        layers = []
        layers.append(ResNetBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResNetBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        features = []
        x = F.relu(self.bn1(self.conv1(x)))
        features.append(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        features.append(x)
        x = self.layer2(x)
        features.append(x)
        x = self.layer3(x)
        features.append(x)
        x = self.layer4(x)
        features.append(x)
        return x, features


class UNetDecoder(nn.Module):
    def __init__(self, out_channels: int = 1):
        super().__init__()
        self.up4 = self._make_upconv_block(512, 256)
        self.up3 = self._make_upconv_block(256 + 256, 128)
        self.up2 = self._make_upconv_block(128 + 128, 64)
        self.up1 = self._make_upconv_block(64 + 64, 64)
        self.final_up = nn.ConvTranspose2d(64 + 64, 32, 2, 2)
        self.final_conv = nn.Sequential(
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, out_channels, 1)
        )

    def _make_upconv_block(self, in_channels: int, out_channels: int):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip_features):
        x = self.up4(x)
        x = torch.cat([x, skip_features[3]], dim=1)
        x = self.up3(x)
        x = torch.cat([x, skip_features[2]], dim=1)
        x = self.up2(x)
        x = torch.cat([x, skip_features[1]], dim=1)
        x = self.up1(x)
        x = torch.cat([x, skip_features[0]], dim=1)
        x = self.final_up(x)
        x = self.final_conv(x)
        return x


class UNetResNetSuperResolution(nn.Module):
    def __init__(self, in_channels: int = 1, out_channels: int = 1, scale_factor: int = 10):
        super().__init__()
        self.scale_factor = scale_factor
        self.encoder = UNetResNetEncoder(in_channels)
        self.decoder = UNetDecoder(out_channels)

        if scale_factor > 1:
            upsampling_layers = []
            current_scale = 1

            while current_scale < scale_factor:
                if scale_factor // current_scale >= 4:
                    factor = 4
                elif scale_factor // current_scale >= 2:
                    factor = 2
                else:
                    factor = scale_factor // current_scale

                upsampling_layers.extend([
                    nn.ConvTranspose2d(out_channels if len(upsampling_layers) == 0 else 32, 32,
                                       factor, factor),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(32, 32, 3, 1, 1),
                    nn.ReLU(inplace=True)
                ])
                current_scale *= factor

            upsampling_layers.append(nn.Conv2d(32, out_channels, 1))
            self.upsampling = nn.Sequential(*upsampling_layers)
        else:
            self.upsampling = nn.Identity()

    def forward(self, x):
        encoded, skip_features = self.encoder(x)
        decoded = self.decoder(encoded, skip_features)
        output = self.upsampling(decoded)
        return output


# ====== LOSS –ò –ú–ï–¢–†–ò–ö–ò ======
class AMSR2SpecificLoss(nn.Module):
    def __init__(self, alpha: float = 1.0, beta: float = 0.15, gamma: float = 0.05):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()

    def gradient_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        def compute_gradients(x):
            grad_x = x[:, :, :-1, :] - x[:, :, 1:, :]
            grad_y = x[:, :, :, :-1] - x[:, :, :, 1:]
            return grad_x, grad_y

        pred_grad_x, pred_grad_y = compute_gradients(pred)
        target_grad_x, target_grad_y = compute_gradients(target)

        loss_x = self.l1_loss(pred_grad_x, target_grad_x)
        loss_y = self.l1_loss(pred_grad_y, target_grad_y)

        return loss_x + loss_y

    def brightness_temperature_consistency(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        pred_mean = torch.mean(pred, dim=[2, 3])
        target_mean = torch.mean(target, dim=[2, 3])
        energy_loss = self.mse_loss(pred_mean, target_mean)

        pred_std = torch.std(pred, dim=[2, 3])
        target_std = torch.std(target, dim=[2, 3])
        distribution_loss = self.mse_loss(pred_std, target_std)

        range_penalty = torch.mean(torch.relu(torch.abs(pred) - 1.0))

        return energy_loss + 0.5 * distribution_loss + 0.1 * range_penalty

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> tuple:
        l1_loss = self.l1_loss(pred, target)
        grad_loss = self.gradient_loss(pred, target)
        phys_loss = self.brightness_temperature_consistency(pred, target)

        total_loss = (self.alpha * l1_loss +
                      self.beta * grad_loss +
                      self.gamma * phys_loss)

        return total_loss, {
            'l1_loss': l1_loss.item(),
            'gradient_loss': grad_loss.item(),
            'physical_loss': phys_loss.item(),
            'total_loss': total_loss.item()
        }


# ====== SEQUENTIAL TRAINER ======
class SequentialAMSR2Trainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ñ–∞–π–ª–∞—Ö"""

    def __init__(self, model: nn.Module, device: torch.device,
                 learning_rate: float = 1e-4, weight_decay: float = 1e-5):
        self.model = model.to(device)
        self.device = device

        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=3, verbose=True
        )

        self.criterion = AMSR2SpecificLoss()
        self.training_history = []
        self.best_loss = float('inf')

    def train_on_file(self, file_path: str, preprocessor, batch_size: int = 2,
                      augment: bool = True, filter_orbit_type: Optional[str] = None):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ"""

        logger.info(f"üìö –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª–µ: {os.path.basename(file_path)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
        memory_monitor.emergency_stop_if_needed()

        # –°–æ–∑–¥–∞–Ω–∏–µ dataset –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        dataset = SingleFileAMSR2Dataset(
            npz_path=file_path,
            preprocessor=preprocessor,
            degradation_scale=4,
            augment=augment,
            filter_orbit_type=filter_orbit_type
        )

        if len(dataset) == 0:
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {file_path}")
            return {'loss': float('inf'), 'swaths': 0}

        # DataLoader —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,  # –ë–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            pin_memory=False
        )

        self.model.train()
        file_losses = []

        try:
            for batch_idx, (low_res, high_res) in enumerate(dataloader):
                if EMERGENCY_STOP:
                    logger.warning("‚ö†Ô∏è –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è")
                    break

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
                if batch_idx % 10 == 0:
                    memory_monitor.emergency_stop_if_needed()

                low_res = low_res.to(self.device)
                high_res = high_res.to(self.device)

                self.optimizer.zero_grad()
                pred = self.model(low_res)
                loss, loss_components = self.criterion(pred, high_res)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                file_losses.append(loss.item())

                # –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
                del low_res, high_res, pred, loss

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ñ–∞–π–ª–µ {file_path}: {e}")
            return {'loss': float('inf'), 'swaths': 0}

        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Ñ–∞–π–ª–∞
        del dataset, dataloader
        memory_monitor.force_cleanup()

        avg_loss = np.mean(file_losses) if file_losses else float('inf')

        logger.info(f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω: loss={avg_loss:.4f}, batches={len(file_losses)}")

        return {'loss': avg_loss, 'swaths': len(dataset)}

    def train_sequential(self, npz_files: List[str], preprocessor,
                         epochs_per_file: int = 1, batch_size: int = 2,
                         save_path: str = "best_amsr2_model.pth"):
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö"""

        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:")
        logger.info(f"   –§–∞–π–ª–æ–≤: {len(npz_files)}")
        logger.info(f"   –≠–ø–æ—Ö –Ω–∞ —Ñ–∞–π–ª: {epochs_per_file}")
        logger.info(f"   Batch size: {batch_size}")

        total_files = len(npz_files)
        processed_files = 0

        for file_idx, file_path in enumerate(npz_files):
            if EMERGENCY_STOP:
                logger.warning("‚ö†Ô∏è –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è")
                break

            logger.info(f"\nüìÇ –§–∞–π–ª {file_idx + 1}/{total_files}: {os.path.basename(file_path)}")

            file_results = []

            # –ù–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –Ω–∞ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ
            for epoch in range(epochs_per_file):
                if EMERGENCY_STOP:
                    break

                logger.info(f"   –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs_per_file}")

                result = self.train_on_file(
                    file_path=file_path,
                    preprocessor=preprocessor,
                    batch_size=batch_size,
                    augment=True
                )

                file_results.append(result)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                if result['loss'] < self.best_loss:
                    self.best_loss = result['loss']
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'loss': self.best_loss,
                        'file_idx': file_idx,
                        'epoch': epoch
                    }, save_path)
                    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: loss={self.best_loss:.4f}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª—É
            avg_file_loss = np.mean([r['loss'] for r in file_results if r['loss'] != float('inf')])
            total_swaths = sum(r['swaths'] for r in file_results)

            self.training_history.append({
                'file_idx': file_idx,
                'file_path': file_path,
                'avg_loss': avg_file_loss,
                'total_swaths': total_swaths,
                'epochs': epochs_per_file
            })

            processed_files += 1

            logger.info(f"üìä –§–∞–π–ª –∑–∞–≤–µ—Ä—à–µ–Ω: avg_loss={avg_file_loss:.4f}, swaths={total_swaths}")
            logger.info(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed_files}/{total_files} —Ñ–∞–π–ª–æ–≤")

            # Scheduler step
            if avg_file_loss != float('inf'):
                self.scheduler.step(avg_file_loss)

        logger.info(f"\nüéâ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}/{total_files}")
        logger.info(f"   –õ—É—á—à–∏–π loss: {self.best_loss:.4f}")

        return self.training_history


# ====== –§–£–ù–ö–¶–ò–ò –£–¢–ò–õ–ò–¢–´ ======
def find_npz_files(directory: str, max_files: Optional[int] = None) -> List[str]:
    """–ü–æ–∏—Å–∫ NPZ —Ñ–∞–π–ª–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"""

    if not os.path.exists(directory):
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory}")
        return []

    pattern = os.path.join(directory, "*.npz")
    all_files = glob.glob(pattern)

    if not all_files:
        logger.error(f"‚ùå NPZ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")
        return []

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    all_files.sort()

    if max_files is not None and max_files > 0:
        selected_files = all_files[:max_files]
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(all_files)} NPZ —Ñ–∞–π–ª–æ–≤, –≤—ã–±—Ä–∞–Ω–æ {len(selected_files)}")
    else:
        selected_files = all_files
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(selected_files)} NPZ —Ñ–∞–π–ª–æ–≤")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤
    total_size_gb = 0
    for file_path in selected_files:
        size_gb = os.path.getsize(file_path) / 1024 ** 3
        total_size_gb += size_gb

    logger.info(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {total_size_gb:.2f} GB")

    if total_size_gb > 10:
        logger.warning(f"‚ö†Ô∏è –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–∞—á–∞—Ç—å —Å –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤")

    return selected_files


def validate_npz_structure(file_path: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã NPZ —Ñ–∞–π–ª–∞"""
    try:
        with np.load(file_path, allow_pickle=True) as data:
            if 'swath_array' not in data:
                return False

            swath_array = data['swath_array']
            if len(swath_array) == 0:
                return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π swath
            first_swath = swath_array[0]
            swath = first_swath.item() if isinstance(first_swath, np.ndarray) else first_swath

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            required_keys = ['temperature', 'metadata']
            for key in required_keys:
                if key not in swath:
                    return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º metadata
            metadata = swath['metadata']
            if 'scale_factor' not in metadata:
                logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç scale_factor –≤ {file_path}")

            return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return False


def estimate_training_time(num_files: int, avg_swaths_per_file: int = 10,
                           epochs_per_file: int = 1, batch_size: int = 2) -> Dict:
    """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è"""

    # –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ (–æ—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ)
    seconds_per_batch = 2.0  # —Å–µ–∫—É–Ω–¥ –Ω–∞ batch
    batches_per_file = avg_swaths_per_file // batch_size

    total_batches = num_files * batches_per_file * epochs_per_file
    total_seconds = total_batches * seconds_per_batch

    return {
        'total_batches': total_batches,
        'estimated_hours': total_seconds / 3600,
        'estimated_days': total_seconds / (3600 * 24),
        'batches_per_file': batches_per_file
    }


def create_training_summary(training_history: List[Dict], save_path: str = "training_summary.json"):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""

    if not training_history:
        return

    valid_results = [h for h in training_history if h['avg_loss'] != float('inf')]

    if not valid_results:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–≤–æ–¥–∫–∏")
        return

    summary = {
        'total_files_processed': len(training_history),
        'successful_files': len(valid_results),
        'total_swaths': sum(h['total_swaths'] for h in valid_results),
        'best_loss': min(h['avg_loss'] for h in valid_results),
        'worst_loss': max(h['avg_loss'] for h in valid_results),
        'average_loss': np.mean([h['avg_loss'] for h in valid_results]),
        'training_history': training_history,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(save_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)

    logger.info(f"üìÑ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
    logger.info(f"   –£—Å–ø–µ—à–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {summary['successful_files']}/{summary['total_files_processed']}")
    logger.info(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ swaths: {summary['total_swaths']}")
    logger.info(f"   –õ—É—á—à–∏–π loss: {summary['best_loss']:.4f}")


def plot_training_progress(training_history: List[Dict], save_path: str = "training_progress.png"):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""

    if not training_history:
        return

    valid_results = [h for h in training_history if h['avg_loss'] != float('inf')]

    if len(valid_results) < 2:
        logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞")
        return

    file_indices = [h['file_idx'] for h in valid_results]
    losses = [h['avg_loss'] for h in valid_results]
    swaths = [h['total_swaths'] for h in valid_results]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # –ì—Ä–∞—Ñ–∏–∫ loss
    ax1.plot(file_indices, losses, 'b-o', linewidth=2, markersize=6)
    ax1.set_title('Training Loss –ø–æ —Ñ–∞–π–ª–∞–º', fontsize=14)
    ax1.set_xlabel('–ù–æ–º–µ—Ä —Ñ–∞–π–ª–∞')
    ax1.set_ylabel('Average Loss')
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')

    # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ swaths
    ax2.bar(file_indices, swaths, alpha=0.7, color='green')
    ax2.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ swaths –≤ —Ñ–∞–π–ª–∞—Ö', fontsize=14)
    ax2.set_xlabel('–ù–æ–º–µ—Ä —Ñ–∞–π–ª–∞')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ swaths')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    logger.info(f"üìà –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")


# ====== MAIN –§–£–ù–ö–¶–ò–Ø ======
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    parser = argparse.ArgumentParser(
        description='AMSR2 Sequential Super-Resolution Training',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã—Ö 10 —Ñ–∞–π–ª–∞—Ö:
   python sequential_amsr2.py --npz-dir /path/to/data --max-files 10

2. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –æ—Ä–±–∏—Ç—ã:
   python sequential_amsr2.py --npz-dir /path/to/data --orbit-filter A

3. –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 5 —Ñ–∞–π–ª–∞—Ö:
   python sequential_amsr2.py --npz-dir /path/to/data --max-files 5 --epochs-per-file 1 --batch-size 1
        '''
    )

    # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--npz-dir', type=str, required=True,
                        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å NPZ —Ñ–∞–π–ª–∞–º–∏')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    parser.add_argument('--max-files', type=int, default=None,
                        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤—Å–µ)')
    parser.add_argument('--orbit-filter', choices=['A', 'D', 'U'],
                        help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –æ—Ä–±–∏—Ç—ã (A=Ascending, D=Descending, U=Unknown)')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument('--epochs-per-file', type=int, default=2,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)')
    parser.add_argument('--batch-size', type=int, default=2,
                        help='–†–∞–∑–º–µ—Ä batch (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1e-4)')
    parser.add_argument('--scale-factor', type=int, default=10,
                        help='–§–∞–∫—Ç–æ—Ä —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏—Å—Ç–µ–º—ã —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
    parser.add_argument('--memory-threshold', type=float, default=70.0,
                        help='–ü–æ—Ä–æ–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 70%%)')
    parser.add_argument('--target-height', type=int, default=2000,
                        help='–¶–µ–ª–µ–≤–∞—è –≤—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è AMSR2 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2000)')
    parser.add_argument('--target-width', type=int, default=420,
                        help='–¶–µ–ª–µ–≤–∞—è —à–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è AMSR2 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 420)')

    # –ü—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    parser.add_argument('--save-path', type=str, default='best_amsr2_sequential_model.pth',
                        help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏')
    parser.add_argument('--no-monitoring', action='store_true',
                        help='–û—Ç–∫–ª—é—á–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏')

    args = parser.parse_args()

    print("üõ∞Ô∏è AMSR2 SEQUENTIAL SUPER-RESOLUTION TRAINER")
    print("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
    memory_info = psutil.virtual_memory()
    logger.info(
        f"üíæ –ü–∞–º—è—Ç—å: {memory_info.available / 1024 ** 3:.1f} GB –¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑ {memory_info.total / 1024 ** 3:.1f} GB ({memory_info.percent:.1f}% –∑–∞–Ω—è—Ç–æ)")

    if memory_info.percent > 60:  # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ —É–∂–µ –ø—Ä–∏ 60%
        logger.warning(f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_info.percent:.1f}%")
        logger.warning("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º")

    if memory_info.available < 4 * 1024 ** 3:  # –ú–∏–Ω–∏–º—É–º 4GB
        logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏: {memory_info.available / 1024 ** 3:.1f} GB")
        logger.error("   –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –º–∏–Ω–∏–º—É–º 4 GB –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        sys.exit(1)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    global memory_monitor
    memory_monitor = MemoryMonitor(args.memory_threshold)

    if not args.no_monitoring:
        memory_monitor.start_monitoring()

    try:
        # –ü–æ–∏—Å–∫ NPZ —Ñ–∞–π–ª–æ–≤
        npz_files = find_npz_files(args.npz_dir, args.max_files)

        if not npz_files:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö NPZ —Ñ–∞–π–ª–æ–≤")
            sys.exit(1)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤...")
        valid_files = []

        for i, file_path in enumerate(npz_files[:min(5, len(npz_files))]):
            if validate_npz_structure(file_path):
                logger.info(f"‚úÖ –§–∞–π–ª {i + 1} –≤–∞–ª–∏–¥–µ–Ω: {os.path.basename(file_path)}")
            else:
                logger.error(f"‚ùå –§–∞–π–ª {i + 1} –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {os.path.basename(file_path)}")

        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        time_estimate = estimate_training_time(
            len(npz_files),
            epochs_per_file=args.epochs_per_file,
            batch_size=args.batch_size
        )

        logger.info(f"‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è:")
        logger.info(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {time_estimate['total_batches']}")
        logger.info(
            f"   –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {time_estimate['estimated_hours']:.1f} —á–∞—Å–æ–≤ ({time_estimate['estimated_days']:.1f} –¥–Ω–µ–π)")

        if time_estimate['estimated_hours'] > 48:
            logger.warning("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏!")
            logger.warning("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–∏—Ç—å --max-files –∏–ª–∏ --epochs-per-file")

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        model = UNetResNetSuperResolution(
            in_channels=1,
            out_channels=1,
            scale_factor=args.scale_factor
        )

        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {total_params:,}")
        logger.info(f"   –§–∞–∫—Ç–æ—Ä —É–≤–µ–ª–∏—á–µ–Ω–∏—è: {args.scale_factor}x")

        # –°–æ–∑–¥–∞–Ω–∏–µ preprocessor
        preprocessor = AMSR2NPZDataPreprocessor(
            target_height=args.target_size,
            target_width=args.target_size
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = SequentialAMSR2Trainer(
            model=model,
            device=device,
            learning_rate=args.lr
        )

        # –í—ã–≤–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
        logger.info(f"   –§–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(npz_files)}")
        logger.info(f"   –≠–ø–æ—Ö –Ω–∞ —Ñ–∞–π–ª: {args.epochs_per_file}")
        logger.info(f"   Batch size: {args.batch_size}")
        logger.info(f"   Learning rate: {args.lr}")
        logger.info(f"   Target —Ä–∞–∑–º–µ—Ä—ã: {args.target_height}x{args.target_width}")
        logger.info(f"   –ü–æ—Ä–æ–≥ –ø–∞–º—è—Ç–∏: {args.memory_threshold}%")

        if args.orbit_filter:
            logger.info(f"   –§–∏–ª—å—Ç—Ä –æ—Ä–±–∏—Ç—ã: {args.orbit_filter}")

        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        try:
            confirm = input("\nüöÄ –ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ? (y/N): ").strip().lower()
            if confirm not in ['y', 'yes', '–¥–∞', '–¥']:
                logger.info("‚ùå –û–±—É—á–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                sys.exit(0)
        except KeyboardInterrupt:
            logger.info("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            sys.exit(0)

        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info("\nüöÄ –ó–∞–ø—É—Å–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        start_time = time.time()

        training_history = trainer.train_sequential(
            npz_files=npz_files,
            preprocessor=preprocessor,
            epochs_per_file=args.epochs_per_file,
            batch_size=args.batch_size,
            save_path=args.save_path
        )

        end_time = time.time()
        training_time = end_time - start_time

        logger.info(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time / 3600:.2f} —á–∞—Å–æ–≤")
        logger.info(f"   –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(training_history)}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
        create_training_summary(training_history)
        plot_training_progress(training_history)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏
        if not args.no_monitoring:
            stats_summary = memory_monitor.get_stats_summary()
            if stats_summary:
                logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
                logger.info(f"   –°—Ä–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞: {stats_summary.get('avg_memory_percent', 0):.1f}%")
                logger.info(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {stats_summary.get('max_memory_percent', 0):.1f}%")
                logger.info(f"   –ú–∏–Ω–∏–º—É–º –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏: {stats_summary.get('min_available_memory_gb', 0):.1f} GB")

        logger.info(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        logger.info(f"   –ú–æ–¥–µ–ª—å: {args.save_path}")
        logger.info(f"   –°–≤–æ–¥–∫–∞: training_summary.json")
        logger.info(f"   –ì—Ä–∞—Ñ–∏–∫: training_progress.png")
        logger.info(f"   –õ–æ–≥: amsr2_sequential.log")

    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except MemoryError as e:
        logger.critical(f"\nüíæ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏: {e}")
    except Exception as e:
        logger.error(f"\n‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        if not args.no_monitoring:
            memory_monitor.stop_monitoring()

        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã")


if __name__ == "__main__":
    main()