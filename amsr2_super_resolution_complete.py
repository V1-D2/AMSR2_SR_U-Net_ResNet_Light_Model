#!/usr/bin/env python3
"""
AMSR2 Brightness Temperature Super-Resolution - Complete Implementation
–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö AMSR2 –≤ 10 —Ä–∞–∑
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ NPZ —Ñ–æ—Ä–º–∞—Ç —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ swath –∏ scale factors
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è Xeon E5-2640 v4 (30 —è–¥–µ—Ä –º–∞–∫—Å–∏–º—É–º)

–ê–≤—Ç–æ—Ä: Volodymyr Didur
–í–µ—Ä—Å–∏—è: 2.1 - CPU Optimized Edition for Xeon
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import glob
from typing import Tuple, List, Optional, Dict
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import logging
from tqdm import tqdm
import time
import json
import shutil
import argparse
from pathlib import Path
import multiprocessing
import psutil

# ====== CPU OPTIMIZATION FOR XEON E5-2640 v4 (30 CORES MAX) ======
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è 30 —è–¥–µ—Ä (–æ—Å—Ç–∞–≤–ª—è–µ–º 10 –¥–ª—è —Å–∏—Å—Ç–µ–º—ã)
MAX_CPU_CORES = 30
os.environ['OMP_NUM_THREADS'] = str(MAX_CPU_CORES)
os.environ['MKL_NUM_THREADS'] = str(MAX_CPU_CORES)
os.environ['NUMEXPR_NUM_THREADS'] = str(MAX_CPU_CORES)

# Intel MKL –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Xeon
os.environ['MKL_DYNAMIC'] = 'FALSE'
os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'

# PyTorch –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
torch.set_num_threads(MAX_CPU_CORES)
if hasattr(torch, 'set_num_interop_threads'):
    torch.set_num_interop_threads(15)  # –ü–æ–ª–æ–≤–∏–Ω–∞ –æ—Ç MAX_CPU_CORES

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –õ–æ–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
logger.info(f"üîß CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Xeon E5-2640 v4:")
logger.info(f"   –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {MAX_CPU_CORES} –∏–∑ {psutil.cpu_count()} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä")
logger.info(f"   –°–≤–æ–±–æ–¥–Ω–æ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã: {psutil.cpu_count() - MAX_CPU_CORES} —è–¥–µ—Ä")
logger.info(f"   –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {psutil.virtual_memory().total / 1024 ** 3:.1f} GB")


# ================================================================================================
# SECTION 1: DATA PREPROCESSING AND LOADING
# ================================================================================================

class AMSR2NPZDataPreprocessor:
    """
    –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è AMSR2 NPZ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ –≤–∞—à–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ swath –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç scale factors
    """

    def __init__(self, target_height: int = None, target_width: int = None):
        self.target_height = target_height
        self.target_width = target_width
        self.stats = {
            'total_files': 0,
            'total_swaths': 0,
            'shapes_found': {},
            'orbit_types': {'A': 0, 'D': 0, 'U': 0},
            'scale_factors': [],
            'temp_ranges': []
        }

    def analyze_npz_files(self, npz_paths: List[str]) -> dict:
        """–ê–Ω–∞–ª–∏–∑ NPZ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""

        logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(npz_paths)} NPZ —Ñ–∞–π–ª–æ–≤...")

        all_shapes = []

        for npz_path in tqdm(npz_paths[:10], desc="–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤"):  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ñ–∞–π–ª–æ–≤
            try:
                with np.load(npz_path, allow_pickle=True) as data:
                    swath_array = data['swath_array']

                    self.stats['total_files'] += 1

                    for swath_dict in swath_array:
                        swath = swath_dict.item() if isinstance(swath_dict, np.ndarray) else swath_dict

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                        temperature = swath['temperature']
                        metadata = swath['metadata']

                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                        shape = temperature.shape
                        all_shapes.append(shape)

                        shape_key = f"{shape[0]}x{shape[1]}"
                        self.stats['shapes_found'][shape_key] = self.stats['shapes_found'].get(shape_key, 0) + 1

                        orbit_type = metadata.get('orbit_type', 'U')
                        self.stats['orbit_types'][orbit_type] += 1

                        scale_factor = metadata.get('scale_factor', 1.0)
                        self.stats['scale_factors'].append(scale_factor)

                        temp_range = metadata.get('temp_range', (0, 0))
                        self.stats['temp_ranges'].append(temp_range)

                        self.stats['total_swaths'] += 1

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {npz_path}: {e}")

        if all_shapes:
            heights = [s[0] for s in all_shapes]
            widths = [s[1] for s in all_shapes]

            analysis = {
                'total_files_analyzed': self.stats['total_files'],
                'total_swaths_found': self.stats['total_swaths'],
                'shapes_distribution': self.stats['shapes_found'],
                'orbit_distribution': self.stats['orbit_types'],
                'height_range': (min(heights), max(heights)),
                'width_range': (min(widths), max(widths)),
                'most_common_shape': max(self.stats['shapes_found'].items(), key=lambda x: x[1]),
                'recommended_size': (min(heights), min(widths)),
                'scale_factor_range': (min(self.stats['scale_factors']), max(self.stats['scale_factors'])),
                'temp_range_summary': {
                    'min_temp': min([tr[0] for tr in self.stats['temp_ranges']]),
                    'max_temp': max([tr[1] for tr in self.stats['temp_ranges']])
                }
            }

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã
            if self.target_height is None:
                self.target_height = analysis['recommended_size'][0]
            if self.target_width is None:
                self.target_width = analysis['recommended_size'][1]

            logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω:")
            logger.info(f"  –ù–∞–π–¥–µ–Ω–æ swath: {analysis['total_swaths_found']}")
            logger.info(f"  –†–∞–∑–º–µ—Ä—ã: {analysis['shapes_distribution']}")
            logger.info(f"  Orbit —Ç–∏–ø—ã: {analysis['orbit_distribution']}")
            logger.info(f"  –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä: {analysis['recommended_size']}")
            logger.info(f"  –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π target: {self.target_height}x{self.target_width}")

            return analysis

        return {}

    def load_swath_from_npz(self, npz_path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö swath –∏–∑ –æ–¥–Ω–æ–≥–æ NPZ —Ñ–∞–π–ª–∞"""

        try:
            with np.load(npz_path, allow_pickle=True) as data:
                swath_array = data['swath_array']
                period_info = data.get('period', 'Unknown period')

                swath_list = []

                for swath_dict in swath_array:
                    swath = swath_dict.item() if isinstance(swath_dict, np.ndarray) else swath_dict

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    raw_temperature = swath['temperature']
                    metadata = swath['metadata']

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º scale factor
                    scale_factor = metadata.get('scale_factor', 1.0)

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –µ–¥–∏–Ω–∏—Ü—ã
                    if raw_temperature.dtype != np.float32:
                        temperature = raw_temperature.astype(np.float32) * scale_factor
                    else:
                        temperature = raw_temperature * scale_factor

                    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ 0 –∏–ª–∏ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ)
                    temperature = np.where(temperature < 50, np.nan, temperature)  # –ù–µ—Ä–µ–∞–ª—å–Ω–æ –Ω–∏–∑–∫–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
                    temperature = np.where(temperature > 350, np.nan, temperature)  # –ù–µ—Ä–µ–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã

                    swath_data = {
                        'temperature': temperature,
                        'metadata': metadata,
                        'file_info': {'source': npz_path, 'period': period_info}
                    }

                    swath_list.append(swath_data)

                return swath_list

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {npz_path}: {e}")
            return []

    def crop_and_pad_to_target(self, temperature: np.ndarray) -> np.ndarray:
        """–û–±—Ä–µ–∑–∫–∞ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ target —Ä–∞–∑–º–µ—Ä–∞"""

        h, w = temperature.shape

        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –±–æ–ª—å—à–µ target - –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Ü–µ–Ω—Ç—Ä—É
        if h > self.target_height:
            start_h = (h - self.target_height) // 2
            temperature = temperature[start_h:start_h + self.target_height]

        if w > self.target_width:
            start_w = (w - self.target_width) // 2
            temperature = temperature[:, start_w:start_w + self.target_width]

        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –º–µ–Ω—å—à–µ target - –¥–æ–ø–æ–ª–Ω—è–µ–º —Å –ø–æ–º–æ—â—å—é reflection padding
        current_h, current_w = temperature.shape

        if current_h < self.target_height or current_w < self.target_width:
            pad_h = max(0, self.target_height - current_h)
            pad_w = max(0, self.target_width - current_w)

            pad_top = pad_h // 2
            pad_bottom = pad_h - pad_top
            pad_left = pad_w // 2
            pad_right = pad_w - pad_left

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º reflection padding –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
            temperature = np.pad(temperature,
                                 ((pad_top, pad_bottom), (pad_left, pad_right)),
                                 mode='reflect')

        return temperature

    def normalize_brightness_temperature(self, temperature: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è brightness temperature —Å —É—á–µ—Ç–æ–º NaN –∑–Ω–∞—á–µ–Ω–∏–π"""

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π
        valid_mask = ~np.isnan(temperature)
        if np.sum(valid_mask) > 0:
            mean_temp = np.mean(temperature[valid_mask])
            temperature = np.where(np.isnan(temperature), mean_temp, temperature)
        else:
            # –ï—Å–ª–∏ –≤—Å–µ NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
            temperature = np.full_like(temperature, 250.0)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-1, 1]
        # –¢–∏–ø–∏—á–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω AMSR2: 50-350K, —Ü–µ–Ω—Ç—Ä –æ–∫–æ–ª–æ 200K
        temperature = np.clip(temperature, 50, 350)
        normalized = (temperature - 200) / 150

        return normalized.astype(np.float32)


class AMSR2NPZDataset(Dataset):
    """Dataset –¥–ª—è AMSR2 NPZ –¥–∞–Ω–Ω—ã—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ swath"""

    def __init__(self, npz_paths: List[str], preprocessor: AMSR2NPZDataPreprocessor,
                 degradation_scale: int = 4, augment: bool = True,
                 filter_orbit_type: Optional[str] = None):

        self.npz_paths = npz_paths
        self.preprocessor = preprocessor
        self.degradation_scale = degradation_scale
        self.augment = augment
        self.filter_orbit_type = filter_orbit_type

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ swath –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
        self.swath_index = []

        logger.info("–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ swath –¥–∞–Ω–Ω—ã—Ö...")

        for npz_path in tqdm(npz_paths, desc="–ó–∞–≥—Ä—É–∑–∫–∞ NPZ —Ñ–∞–π–ª–æ–≤"):
            swath_list = preprocessor.load_swath_from_npz(npz_path)

            for swath_idx, swath in enumerate(swath_list):
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –æ—Ä–±–∏—Ç—ã –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
                if filter_orbit_type is not None:
                    orbit_type = swath['metadata'].get('orbit_type', 'U')
                    if orbit_type != filter_orbit_type:
                        continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                temp = swath['temperature']
                valid_pixels = np.sum(~np.isnan(temp))
                total_pixels = temp.size

                if valid_pixels / total_pixels > 0.1:  # –ú–∏–Ω–∏–º—É–º 10% –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π
                    self.swath_index.append({
                        'npz_path': npz_path,
                        'swath_idx': swath_idx,
                        'swath_data': swath
                    })

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(self.swath_index)} –≤–∞–ª–∏–¥–Ω—ã—Ö swath")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –µ—Å–ª–∏ –µ—â–µ –Ω–µ –±—ã–ª–æ
        if not hasattr(preprocessor, 'target_height') or preprocessor.target_height is None:
            self.preprocessor.analyze_npz_files(npz_paths)

    def __len__(self):
        return len(self.swath_index)

    def create_degradation(self, high_res: np.ndarray) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –¥–ª—è self-supervised –æ–±—É—á–µ–Ω–∏—è"""

        h, w = high_res.shape

        # 1. Downsample
        low_res = F.interpolate(
            torch.from_numpy(high_res).unsqueeze(0).unsqueeze(0),
            size=(h // self.degradation_scale, w // self.degradation_scale),
            mode='bilinear',
            align_corners=False
        )

        # 2. –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —à—É–º (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π –¥–ª—è AMSR2)
        noise_std = 0.01  # –ú–µ–Ω—å—à–µ —à—É–º–∞ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        noise = torch.randn_like(low_res) * noise_std
        low_res = low_res + noise

        # 3. –õ–µ–≥–∫–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ (PSF —Å–µ–Ω—Å–æ—Ä–∞)
        blur = transforms.GaussianBlur(kernel_size=3, sigma=0.3)
        low_res = blur(low_res)

        # 4. Upsampling –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        degraded = F.interpolate(
            low_res,
            size=(h, w),
            mode='bilinear',
            align_corners=False
        )

        return degraded.squeeze().numpy()

    def augment_data(self, data: np.ndarray) -> np.ndarray:
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥–µ–æ—Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤"""

        if not self.augment:
            return data

        # –¢–æ–ª—å–∫–æ safe –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        # –°–ª—É—á–∞–π–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ (–≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ)
        if np.random.rand() > 0.5:
            data = np.fliplr(data)

        if np.random.rand() > 0.5:
            data = np.flipud(data)

        # –ü–æ–≤–æ—Ä–æ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ 90 –≥—Ä–∞–¥—É—Å–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç grid —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
        if np.random.rand() > 0.7:
            k = np.random.choice([1, 2, 3])
            data = np.rot90(data, k)

        return data

    def __getitem__(self, idx):

        swath_info = self.swath_index[idx]
        swath_data = swath_info['swath_data']

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
        temperature = swath_data['temperature'].copy()

        # –û–±—Ä–µ–∑–∫–∞/–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ target —Ä–∞–∑–º–µ—Ä–∞
        temperature = self.preprocessor.crop_and_pad_to_target(temperature)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        temperature = self.preprocessor.normalize_brightness_temperature(temperature)

        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        temperature = self.augment_data(temperature)

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        degraded = self.create_degradation(temperature)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        high_res = torch.from_numpy(temperature).unsqueeze(0).float()
        low_res = torch.from_numpy(degraded).unsqueeze(0).float()

        return low_res, high_res


# ================================================================================================
# SECTION 2: NEURAL NETWORK ARCHITECTURE
# ================================================================================================

class ResNetBlock(nn.Module):
    """ResNet –±–ª–æ–∫ –¥–ª—è encoder"""

    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class UNetResNetEncoder(nn.Module):
    """ResNet-based encoder –¥–ª—è U-Net"""

    def __init__(self, in_channels: int = 1):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, 64, 7, 2, 3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(3, 2, 1)

        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)

    def _make_layer(self, in_channels: int, out_channels: int, blocks: int, stride: int):
        layers = []
        layers.append(ResNetBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResNetBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        features = []

        x = F.relu(self.bn1(self.conv1(x)))
        features.append(x)

        x = self.maxpool(x)
        x = self.layer1(x)
        features.append(x)

        x = self.layer2(x)
        features.append(x)

        x = self.layer3(x)
        features.append(x)

        x = self.layer4(x)
        features.append(x)

        return x, features


class UNetDecoder(nn.Module):
    """Decoder –¥–ª—è U-Net —Å upsampling"""

    def __init__(self, out_channels: int = 1):
        super().__init__()

        self.up4 = self._make_upconv_block(512, 256)
        self.up3 = self._make_upconv_block(256 + 256, 128)
        self.up2 = self._make_upconv_block(128 + 128, 64)
        self.up1 = self._make_upconv_block(64 + 64, 64)

        self.final_up = nn.ConvTranspose2d(64 + 64, 32, 2, 2)
        self.final_conv = nn.Sequential(
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, out_channels, 1)
        )

    def _make_upconv_block(self, in_channels: int, out_channels: int):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip_features):
        x = self.up4(x)
        x = torch.cat([x, skip_features[3]], dim=1)

        x = self.up3(x)
        x = torch.cat([x, skip_features[2]], dim=1)

        x = self.up2(x)
        x = torch.cat([x, skip_features[1]], dim=1)

        x = self.up1(x)
        x = torch.cat([x, skip_features[0]], dim=1)

        x = self.final_up(x)
        x = self.final_conv(x)

        return x


class UNetResNetSuperResolution(nn.Module):
    """U-Net —Å ResNet backbone –¥–ª—è AMSR2 super-resolution"""

    def __init__(self, in_channels: int = 1, out_channels: int = 1, scale_factor: int = 10):
        super().__init__()

        self.scale_factor = scale_factor
        self.encoder = UNetResNetEncoder(in_channels)
        self.decoder = UNetDecoder(out_channels)

        # Progressive upsampling –¥–ª—è –±–æ–ª—å—à–∏—Ö scale factors
        if scale_factor > 1:
            upsampling_layers = []
            current_scale = 1

            while current_scale < scale_factor:
                if scale_factor // current_scale >= 4:
                    factor = 4
                elif scale_factor // current_scale >= 2:
                    factor = 2
                else:
                    factor = scale_factor // current_scale

                upsampling_layers.extend([
                    nn.ConvTranspose2d(out_channels if len(upsampling_layers) == 0 else 32, 32,
                                       factor, factor),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(32, 32, 3, 1, 1),
                    nn.ReLU(inplace=True)
                ])
                current_scale *= factor

            upsampling_layers.append(nn.Conv2d(32, out_channels, 1))
            self.upsampling = nn.Sequential(*upsampling_layers)
        else:
            self.upsampling = nn.Identity()

    def forward(self, x):
        encoded, skip_features = self.encoder(x)
        decoded = self.decoder(encoded, skip_features)
        output = self.upsampling(decoded)
        return output


# ================================================================================================
# SECTION 3: LOSS FUNCTIONS AND METRICS
# ================================================================================================

class AMSR2SpecificLoss(nn.Module):
    """Loss —Ñ—É–Ω–∫—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è AMSR2 brightness temperature"""

    def __init__(self, alpha: float = 1.0, beta: float = 0.15, gamma: float = 0.05):
        super().__init__()
        self.alpha = alpha  # L1 loss
        self.beta = beta  # Gradient loss
        self.gamma = gamma  # Physical consistency

        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()

    def gradient_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤–∞–∂–Ω–æ –¥–ª—è temperature —Å—Ç—Ä—É–∫—Ç—É—Ä"""

        def compute_gradients(x):
            grad_x = x[:, :, :-1, :] - x[:, :, 1:, :]
            grad_y = x[:, :, :, :-1] - x[:, :, :, 1:]
            return grad_x, grad_y

        pred_grad_x, pred_grad_y = compute_gradients(pred)
        target_grad_x, target_grad_y = compute_gradients(target)

        loss_x = self.l1_loss(pred_grad_x, target_grad_x)
        loss_y = self.l1_loss(pred_grad_y, target_grad_y)

        return loss_x + loss_y

    def brightness_temperature_consistency(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ brightness temperature"""

        # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        pred_mean = torch.mean(pred, dim=[2, 3])
        target_mean = torch.mean(target, dim=[2, 3])
        energy_loss = self.mse_loss(pred_mean, target_mean)

        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä
        pred_std = torch.std(pred, dim=[2, 3])
        target_std = torch.std(target, dim=[2, 3])
        distribution_loss = self.mse_loss(pred_std, target_std)

        # 3. Penalize unrealistic values (outside normalized range)
        range_penalty = torch.mean(torch.relu(torch.abs(pred) - 1.0))

        return energy_loss + 0.5 * distribution_loss + 0.1 * range_penalty

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> tuple:
        l1_loss = self.l1_loss(pred, target)
        grad_loss = self.gradient_loss(pred, target)
        phys_loss = self.brightness_temperature_consistency(pred, target)

        total_loss = (self.alpha * l1_loss +
                      self.beta * grad_loss +
                      self.gamma * phys_loss)

        return total_loss, {
            'l1_loss': l1_loss.item(),
            'gradient_loss': grad_loss.item(),
            'physical_loss': phys_loss.item(),
            'total_loss': total_loss.item()
        }


class SuperResolutionMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ super-resolution"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.psnr_values = []
        self.ssim_values = []
        self.mse_values = []
        self.mae_values = []

    def psnr(self, pred: np.ndarray, target: np.ndarray, max_val: float = 2.0) -> float:
        """Peak Signal-to-Noise Ratio"""
        mse = np.mean((pred - target) ** 2)
        if mse == 0:
            return float('inf')
        return 20 * np.log10(max_val / np.sqrt(mse))

    def ssim(self, pred: np.ndarray, target: np.ndarray) -> float:
        """Simplified SSIM calculation"""
        mu1 = np.mean(pred)
        mu2 = np.mean(target)

        sigma1_sq = np.var(pred)
        sigma2_sq = np.var(target)
        sigma12 = np.mean((pred - mu1) * (target - mu2))

        c1 = 0.01 ** 2
        c2 = 0.03 ** 2

        numerator = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)
        denominator = (mu1 ** 2 + mu2 ** 2 + c1) * (sigma1_sq + sigma2_sq + c2)

        return numerator / denominator

    def update(self, pred: torch.Tensor, target: torch.Tensor):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        pred_np = pred.detach().cpu().numpy()
        target_np = target.detach().cpu().numpy()

        batch_size = pred_np.shape[0]

        for i in range(batch_size):
            p = pred_np[i, 0]  # –£–±–∏—Ä–∞–µ–º channel dimension
            t = target_np[i, 0]

            self.psnr_values.append(self.psnr(p, t))
            self.ssim_values.append(self.ssim(p, t))
            self.mse_values.append(mean_squared_error(t.flatten(), p.flatten()))
            self.mae_values.append(mean_absolute_error(t.flatten(), p.flatten()))

    def get_metrics(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        if not self.psnr_values:
            return {}

        return {
            'PSNR': np.mean(self.psnr_values),
            'SSIM': np.mean(self.ssim_values),
            'MSE': np.mean(self.mse_values),
            'MAE': np.mean(self.mae_values),
            'PSNR_std': np.std(self.psnr_values),
            'SSIM_std': np.std(self.ssim_values)
        }


# ================================================================================================
# SECTION 4: TRAINING INFRASTRUCTURE
# ================================================================================================

class AMSR2SuperResolutionTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ super-resolution"""

    def __init__(self, model: nn.Module, device: torch.device,
                 learning_rate: float = 1e-4, weight_decay: float = 1e-5):
        self.model = model.to(device)
        self.device = device

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ scheduler
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )

        # Loss —Ñ—É–Ω–∫—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
        self.criterion = AMSR2SpecificLoss()
        self.metrics = SuperResolutionMetrics()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        self.train_losses = []
        self.val_losses = []
        self.train_metrics = []
        self.val_metrics = []

        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        self.best_val_loss = float('inf')
        self.best_model_state = None

    def train_epoch(self, train_loader: DataLoader) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º CPU"""
        self.model.train()
        epoch_losses = []
        self.metrics.reset()

        # CPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        cpu_monitoring = self.device.type == 'cpu'
        if cpu_monitoring:
            try:
                import psutil
                cpu_percent_start = psutil.cpu_percent()
                memory_start = psutil.virtual_memory().percent
                process = psutil.Process()
                start_threads = process.num_threads()
                logger.info(
                    f"üîß –ù–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏ - CPU: {cpu_percent_start:.1f}%, RAM: {memory_start:.1f}%, Threads: {start_threads}")
            except ImportError:
                cpu_monitoring = False
                logger.warning("psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ CPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        progress_bar = tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ")

        for batch_idx, (low_res, high_res) in enumerate(progress_bar):
            low_res = low_res.to(self.device)
            high_res = high_res.to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            pred = self.model(low_res)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
            loss, loss_components = self.criterion(pred, high_res)

            # Backward pass
            loss.backward()

            # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            epoch_losses.append(loss.item())
            self.metrics.update(pred, high_res)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ progress bar —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            if batch_idx % 10 == 0:
                current_metrics = self.metrics.get_metrics()
                postfix = {
                    'Loss': f"{loss.item():.4f}",
                    'PSNR': f"{current_metrics.get('PSNR', 0):.2f}",
                    'SSIM': f"{current_metrics.get('SSIM', 0):.3f}"
                }

                # –î–æ–±–∞–≤–ª—è–µ–º CPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                if cpu_monitoring and batch_idx % 50 == 0:
                    try:
                        cpu_usage = psutil.cpu_percent(interval=0.1)
                        memory_usage = psutil.virtual_memory().percent
                        process_threads = process.num_threads()

                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ progress bar —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        postfix['CPU'] = f"{cpu_usage:.0f}%"
                        postfix['RAM'] = f"{memory_usage:.0f}%"

                        # –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –∫–∞–∂–¥—ã–µ 100 –±–∞—Ç—á–µ–π
                        if batch_idx % 100 == 0:
                            logger.info(
                                f"Batch {batch_idx}: CPU {cpu_usage:.1f}%, RAM {memory_usage:.1f}%, Threads {process_threads}")
                    except:
                        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

                progress_bar.set_postfix(postfix)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–æ—Ö–∏
        if cpu_monitoring:
            try:
                cpu_percent_end = psutil.cpu_percent()
                memory_end = psutil.virtual_memory().percent
                end_threads = process.num_threads()
                logger.info(
                    f"üèÅ –ö–æ–Ω–µ—Ü —ç–ø–æ—Ö–∏ - CPU: {cpu_percent_end:.1f}%, RAM: {memory_end:.1f}%, Threads: {end_threads}")
            except:
                pass

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
        epoch_metrics = self.metrics.get_metrics()
        avg_loss = np.mean(epoch_losses)

        return {
            'avg_loss': avg_loss,
            'metrics': epoch_metrics,
            'loss_components': loss_components
        }

    def validate_epoch(self, val_loader: DataLoader) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.model.eval()
        epoch_losses = []
        self.metrics.reset()

        with torch.no_grad():
            for low_res, high_res in tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è"):
                low_res = low_res.to(self.device)
                high_res = high_res.to(self.device)

                # Forward pass
                pred = self.model(low_res)
                loss, loss_components = self.criterion(pred, high_res)

                epoch_losses.append(loss.item())
                self.metrics.update(pred, high_res)

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
        epoch_metrics = self.metrics.get_metrics()
        avg_loss = np.mean(epoch_losses)

        return {
            'avg_loss': avg_loss,
            'metrics': epoch_metrics,
            'loss_components': loss_components
        }

    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              num_epochs: int, save_path: str = "best_amsr2_model.pth"):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""

        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö")
        logger.info(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()):,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

        start_time = time.time()

        for epoch in range(num_epochs):
            epoch_start = time.time()

            # –û–±—É—á–µ–Ω–∏–µ
            train_results = self.train_epoch(train_loader)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_results = self.validate_epoch(val_loader)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
            self.train_losses.append(train_results['avg_loss'])
            self.val_losses.append(val_results['avg_loss'])
            self.train_metrics.append(train_results['metrics'])
            self.val_metrics.append(val_results['metrics'])

            # Scheduler step
            self.scheduler.step(val_results['avg_loss'])

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_results['avg_loss'] < self.best_val_loss:
                self.best_val_loss = val_results['avg_loss']
                self.best_model_state = self.model.state_dict().copy()
                torch.save({
                    'model_state_dict': self.best_model_state,
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': self.best_val_loss,
                    'train_metrics': self.train_metrics,
                    'val_metrics': self.val_metrics
                }, save_path)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å val_loss: {self.best_val_loss:.4f}")

            epoch_time = time.time() - epoch_start

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏
            logger.info(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs} ({epoch_time:.1f}s)")
            logger.info(f"Train Loss: {train_results['avg_loss']:.4f}")
            logger.info(f"Val Loss: {val_results['avg_loss']:.4f}")

            if train_results['metrics']:
                logger.info(
                    f"Train PSNR: {train_results['metrics']['PSNR']:.2f} ¬± {train_results['metrics']['PSNR_std']:.2f}")
                logger.info(
                    f"Train SSIM: {train_results['metrics']['SSIM']:.3f} ¬± {train_results['metrics']['SSIM_std']:.3f}")

            if val_results['metrics']:
                logger.info(
                    f"Val PSNR: {val_results['metrics']['PSNR']:.2f} ¬± {val_results['metrics']['PSNR_std']:.2f}")
                logger.info(
                    f"Val SSIM: {val_results['metrics']['SSIM']:.3f} ¬± {val_results['metrics']['SSIM_std']:.3f}")

        total_time = time.time() - start_time
        logger.info(f"\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time / 3600:.1f} —á–∞—Å–æ–≤")
        logger.info(f"–õ—É—á—à–∏–π val_loss: {self.best_val_loss:.4f}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        self.model.load_state_dict(self.best_model_state)

        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'best_val_loss': self.best_val_loss
        }

    def plot_training_history(self, save_path: str = "training_history.png"):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        epochs = range(1, len(self.train_losses) + 1)

        # Loss –≥—Ä–∞—Ñ–∏–∫
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Train Loss')
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Val Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # PSNR –≥—Ä–∞—Ñ–∏–∫
        if self.train_metrics and self.val_metrics:
            train_psnr = [m.get('PSNR', 0) for m in self.train_metrics]
            val_psnr = [m.get('PSNR', 0) for m in self.val_metrics]

            axes[0, 1].plot(epochs, train_psnr, 'b-', label='Train PSNR')
            axes[0, 1].plot(epochs, val_psnr, 'r-', label='Val PSNR')
            axes[0, 1].set_title('PSNR Over Time')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('PSNR (dB)')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # SSIM –≥—Ä–∞—Ñ–∏–∫
            train_ssim = [m.get('SSIM', 0) for m in self.train_metrics]
            val_ssim = [m.get('SSIM', 0) for m in self.val_metrics]

            axes[1, 0].plot(epochs, train_ssim, 'b-', label='Train SSIM')
            axes[1, 0].plot(epochs, val_ssim, 'r-', label='Val SSIM')
            axes[1, 0].set_title('SSIM Over Time')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('SSIM')
            axes[1, 0].legend()
            axes[1, 0].grid(True)

            # MAE –≥—Ä–∞—Ñ–∏–∫
            train_mae = [m.get('MAE', 0) for m in self.train_metrics]
            val_mae = [m.get('MAE', 0) for m in self.val_metrics]

            axes[1, 1].plot(epochs, train_mae, 'b-', label='Train MAE')
            axes[1, 1].plot(epochs, val_mae, 'r-', label='Val MAE')
            axes[1, 1].set_title('MAE Over Time')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('MAE')
            axes[1, 1].legend()
            axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        logger.info(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ {save_path}")


# ================================================================================================
# SECTION 5: DATA LOADING AND UTILITIES
# ================================================================================================

def create_amsr2_data_loaders(npz_dir: str, batch_size: int = 6,
                              val_split: float = 0.2, num_workers: int = 15,  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 30 —è–¥–µ—Ä
                              filter_orbit_type: Optional[str] = None) -> tuple:
    """–°–æ–∑–¥–∞–Ω–∏–µ data loaders –¥–ª—è NPZ —Ñ–∞–π–ª–æ–≤ AMSR2 —Å CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""

    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö NPZ —Ñ–∞–π–ª–æ–≤
    npz_pattern = os.path.join(npz_dir, "*.npz")
    npz_files = glob.glob(npz_pattern)

    if not npz_files:
        raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ NPZ —Ñ–∞–π–ª–æ–≤ –≤ {npz_dir}")

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(npz_files)} NPZ —Ñ–∞–π–ª–æ–≤")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    split_idx = int(len(npz_files) * (1 - val_split))
    train_files = npz_files[:split_idx]
    val_files = npz_files[split_idx:]

    logger.info(f"Train files: {len(train_files)}")
    logger.info(f"Val files: {len(val_files)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ preprocessor —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö
    preprocessor = AMSR2NPZDataPreprocessor()
    analysis = preprocessor.analyze_npz_files(npz_files[:5])  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤

    # –°–æ–∑–¥–∞–Ω–∏–µ datasets
    train_dataset = AMSR2NPZDataset(
        npz_paths=train_files,
        preprocessor=preprocessor,
        degradation_scale=4,
        augment=True,
        filter_orbit_type=filter_orbit_type
    )

    val_dataset = AMSR2NPZDataset(
        npz_paths=val_files,
        preprocessor=preprocessor,
        degradation_scale=4,
        augment=False,
        filter_orbit_type=filter_orbit_type
    )

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ workers –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã
    effective_workers = min(num_workers, 15)  # –ú–∞–∫—Å–∏–º—É–º 15 workers

    logger.info(f"üîß DataLoader –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    logger.info(f"   Workers: {effective_workers} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)")
    logger.info(f"   Batch size: {batch_size}")
    logger.info(f"   Pin memory: False (CPU mode)")

    # –°–æ–∑–¥–∞–Ω–∏–µ data loaders —Å CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=effective_workers,
        pin_memory=False,  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è CPU
        drop_last=True,
        persistent_workers=True  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=effective_workers,
        pin_memory=False,  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è CPU
        persistent_workers=True
    )

    return train_loader, val_loader, preprocessor, analysis


def create_test_npz_files(output_dir: str, num_files: int = 5):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö NPZ —Ñ–∞–π–ª–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ AMSR2 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""

    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ {num_files} —Ç–µ—Å—Ç–æ–≤—ã—Ö NPZ —Ñ–∞–π–ª–æ–≤...")

    for file_idx in range(num_files):
        swath_array = []

        # –ö–∞–∂–¥—ã–π —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ swath
        num_swaths = np.random.randint(5, 15)

        for swath_idx in range(num_swaths):
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            height = np.random.choice([2041, 2036, 2000, 1950])
            width = np.random.choice([421, 420, 422])

            # –°–æ–∑–¥–∞–µ–º raw temperature –¥–∞–Ω–Ω—ã–µ (–¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è scale factor)
            # –¢–∏–ø–∏—á–Ω—ã–µ raw –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ scaling
            raw_temp = np.random.randint(500, 3000, size=(height, width), dtype=np.uint16)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            x, y = np.meshgrid(np.linspace(0, 10, width), np.linspace(0, 20, height))
            structures = (200 * np.sin(x / 2) * np.cos(y / 3)).astype(np.int16)
            raw_temp = (raw_temp.astype(np.int32) + structures).astype(np.uint16)

            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –≤ –≤–∞—à–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ
            scale_factor = 0.1  # –¢–∏–ø–∏—á–Ω—ã–π scale factor –¥–ª—è AMSR2
            orbit_type = np.random.choice(['A', 'D', 'U'])

            # –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è scale factor –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ BT
            actual_temp = raw_temp.astype(np.float32) * scale_factor
            temp_range = (int(np.min(actual_temp)), int(np.max(actual_temp)))

            metadata = {
                'orbit_type': orbit_type,
                'scale_factor': scale_factor,
                'temp_range': temp_range,
                'shape': raw_temp.shape
            }

            swath_dict = {
                'temperature': raw_temp,  # Raw data –∫–∞–∫ –≤ –≤–∞—à–µ–º —Ñ–æ—Ä–º–∞—Ç–µ
                'metadata': metadata
            }

            swath_array.append(swath_dict)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ NPZ —Ñ–æ—Ä–º–∞—Ç–µ
        period_info = f"2024-01-{file_idx + 1:02d}T00:00:00 to 2024-01-{file_idx + 1:02d}T23:59:59"

        save_dict = {
            'swath_array': np.array(swath_array, dtype=object),
            'period': period_info
        }

        output_file = os.path.join(output_dir, f"AMSR2_temp_only_test_{file_idx:03d}.npz")
        np.savez_compressed(output_file, **save_dict)

        logger.info(f"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {output_file} ({num_swaths} swaths)")


def test_model_inference(model: nn.Module, test_loader: DataLoader,
                         device: torch.device, save_examples: bool = True,
                         preprocessor: AMSR2NPZDataPreprocessor = None):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π results –¥–ª—è AMSR2"""

    model.eval()
    metrics = SuperResolutionMetrics()

    example_count = 0
    max_examples = 5

    with torch.no_grad():
        for batch_idx, (low_res, high_res) in enumerate(test_loader):
            low_res = low_res.to(device)
            high_res = high_res.to(device)

            # Inference
            start_time = time.time()
            pred = model(low_res)
            inference_time = time.time() - start_time

            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics.update(pred, high_res)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
            if save_examples and example_count < max_examples:
                batch_size = low_res.shape[0]
                for i in range(min(batch_size, max_examples - example_count)):

                    lr_img = low_res[i, 0].cpu().numpy()
                    hr_img = high_res[i, 0].cpu().numpy()
                    pred_img = pred[i, 0].cpu().numpy()

                    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π BT
                    lr_bt = (lr_img * 150) + 200  # –û–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    hr_bt = (hr_img * 150) + 200
                    pred_bt = (pred_img * 150) + 200

                    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

                    # Input (degraded)
                    im1 = axes[0].imshow(lr_bt, cmap='coolwarm', vmin=150, vmax=300, aspect='auto')
                    axes[0].set_title(f'Input (Degraded)\n{lr_bt.shape}\nRange: {lr_bt.min():.1f}-{lr_bt.max():.1f}K')
                    axes[0].axis('off')
                    plt.colorbar(im1, ax=axes[0], label='Brightness Temperature (K)')

                    # Target (original)
                    im2 = axes[1].imshow(hr_bt, cmap='coolwarm', vmin=150, vmax=300, aspect='auto')
                    axes[1].set_title(f'Target (Original)\n{hr_bt.shape}\nRange: {hr_bt.min():.1f}-{hr_bt.max():.1f}K')
                    axes[1].axis('off')
                    plt.colorbar(im2, ax=axes[1], label='Brightness Temperature (K)')

                    # Enhanced
                    im3 = axes[2].imshow(pred_bt, cmap='coolwarm', vmin=150, vmax=300, aspect='auto')
                    axes[2].set_title(
                        f'Enhanced (10x SR)\n{pred_bt.shape}\nRange: {pred_bt.min():.1f}-{pred_bt.max():.1f}K')
                    axes[2].axis('off')
                    plt.colorbar(im3, ax=axes[2], label='Brightness Temperature (K)')

                    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
                    psnr_val = metrics.psnr(pred_img, hr_img)
                    ssim_val = metrics.ssim(pred_img, hr_img)

                    plt.suptitle(
                        f'AMSR2 Super-Resolution Example {example_count + 1}\nPSNR: {psnr_val:.2f}dB, SSIM: {ssim_val:.3f}',
                        fontsize=14)
                    plt.tight_layout()
                    plt.savefig(f'amsr2_example_{example_count + 1}.png', dpi=300, bbox_inches='tight')
                    plt.show()

                    example_count += 1

                    if example_count >= max_examples:
                        break

            if batch_idx == 0:
                logger.info(f"–í—Ä–µ–º—è inference –¥–ª—è batch {low_res.shape}: {inference_time:.3f}s")

            if example_count >= max_examples:
                break

    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final_metrics = metrics.get_metrics()

    logger.info(f"\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø AMSR2 ===")
    logger.info(f"PSNR: {final_metrics['PSNR']:.2f} ¬± {final_metrics['PSNR_std']:.2f} dB")
    logger.info(f"SSIM: {final_metrics['SSIM']:.3f} ¬± {final_metrics['SSIM_std']:.3f}")
    logger.info(f"MSE: {final_metrics['MSE']:.6f}")
    logger.info(f"MAE: {final_metrics['MAE']:.6f}")

    return final_metrics


def process_new_npz_file(model_path: str, npz_path: str, output_path: str,
                         swath_index: int = None, orbit_filter: str = None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ NPZ —Ñ–∞–π–ª–∞ —Å trained –º–æ–¥–µ–ª—å—é"""

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ NPZ —Ñ–∞–π–ª–∞: {npz_path}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = UNetResNetSuperResolution()

    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    preprocessor = AMSR2NPZDataPreprocessor()
    swath_list = preprocessor.load_swath_from_npz(npz_path)

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(swath_list)} swath –∏–∑ —Ñ–∞–π–ª–∞")

    enhanced_swaths = []

    for idx, swath in enumerate(swath_list):
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if swath_index is not None and idx != swath_index:
            continue

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –æ—Ä–±–∏—Ç—ã –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if orbit_filter is not None:
            orbit_type = swath['metadata'].get('orbit_type', 'U')
            if orbit_type != orbit_filter:
                continue

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ swath {idx}: {swath['metadata']}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        temperature = swath['temperature'].copy()
        original_shape = temperature.shape

        # –ü—Ä–∏–º–µ–Ω—è–µ–º scale factor –µ—Å–ª–∏ –µ—Å—Ç—å raw –¥–∞–Ω–Ω—ã–µ
        scale_factor = swath['metadata'].get('scale_factor', 1.0)
        if temperature.dtype != np.float32:
            temperature = temperature.astype(np.float32) * scale_factor

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        temperature = np.where(temperature < 50, np.nan, temperature)
        temperature = np.where(temperature > 350, np.nan, temperature)

        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        if preprocessor.target_height is None:
            preprocessor.target_height, preprocessor.target_width = temperature.shape

        processed_temp = preprocessor.crop_and_pad_to_target(temperature)
        normalized_temp = preprocessor.normalize_brightness_temperature(processed_temp)

        # Inference
        input_tensor = torch.from_numpy(normalized_temp).unsqueeze(0).unsqueeze(0).float().to(device)

        with torch.no_grad():
            enhanced = model(input_tensor)

        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        enhanced_np = enhanced.squeeze().cpu().numpy()
        enhanced_bt = (enhanced_np * 150) + 200  # –û–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

        enhanced_swaths.append({
            'original_shape': original_shape,
            'enhanced_shape': enhanced_bt.shape,
            'enhanced_temperature': enhanced_bt,
            'metadata': swath['metadata'],
            'enhancement_info': {
                'model_used': model_path,
                'scale_factor_applied': 10,
                'processing_date': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        })

        logger.info(f"  Enhanced: {original_shape} ‚Üí {enhanced_bt.shape}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if enhanced_swaths:
        np.savez_compressed(output_path, enhanced_swaths=enhanced_swaths)
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enhanced_swaths)} swath")
    else:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

    return enhanced_swaths


# ================================================================================================
# SECTION 6: MAIN EXECUTION
# ================================================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å NPZ –¥–∞–Ω–Ω—ã–º–∏ - CPU –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""

    parser = argparse.ArgumentParser(description='AMSR2 Super-Resolution Training - Xeon Optimized')

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--npz-dir', type=str, default='./npz_data',
                        help='–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å NPZ —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--batch-size', type=int, default=6,
                        help='–†–∞–∑–º–µ—Ä batch (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 30 —è–¥–µ—Ä)')
    parser.add_argument('--epochs', type=int, default=50,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate')
    parser.add_argument('--scale-factor', type=int, default=10,
                        help='–ú–∞—Å—à—Ç–∞–± —É–≤–µ–ª–∏—á–µ–Ω–∏—è')
    parser.add_argument('--orbit-filter', choices=['A', 'D', 'U'],
                        help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –æ—Ä–±–∏—Ç—ã')
    parser.add_argument('--test-mode', action='store_true',
                        help='–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏')

    # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--cpu-workers', type=int, default=15,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU workers –¥–ª—è DataLoader (–º–∞–∫—Å 15)')
    parser.add_argument('--cpu-threads', type=int, default=30,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU threads –¥–ª—è PyTorch (–º–∞–∫—Å 30)')
    parser.add_argument('--max-cpu-cores', type=int, default=30,
                        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU —è–¥–µ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--val-split', type=float, default=0.2,
                        help='–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
    parser.add_argument('--save-path', type=str, default='best_amsr2_model.pth',
                        help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏')

    args = parser.parse_args()

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π CPU
    max_cores = min(args.max_cpu_cores, 30)  # –ñ–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ 30 —è–¥–µ—Ä
    max_workers = min(args.cpu_workers, 15)  # –ñ–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ 15 workers

    if args.cpu_threads:
        actual_threads = min(args.cpu_threads, max_cores)
        torch.set_num_threads(actual_threads)
        os.environ['OMP_NUM_THREADS'] = str(actual_threads)
        os.environ['MKL_NUM_THREADS'] = str(actual_threads)
        logger.info(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ CPU threads: {actual_threads}")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    BATCH_SIZE = args.batch_size
    NUM_EPOCHS = args.epochs
    LEARNING_RATE = args.lr
    SCALE_FACTOR = args.scale_factor

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    logger.info(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    logger.info(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è: batch_size={BATCH_SIZE}, epochs={NUM_EPOCHS}, lr={LEARNING_RATE}")

    if DEVICE.type == 'cpu':
        logger.info("üîß CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Xeon E5-2640 v4:")
        logger.info(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —è–¥–µ—Ä: {max_cores} –∏–∑ {psutil.cpu_count()} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö")
        logger.info(f"   DataLoader workers: {max_workers}")
        logger.info(f"   –°–∏—Å—Ç–µ–º–µ –æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {psutil.cpu_count() - max_cores} —è–¥–µ—Ä")
        logger.info(f"   –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {psutil.virtual_memory().total / 1024 ** 3:.1f} GB")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        if BATCH_SIZE > 8:
            logger.warning(f"‚ö†Ô∏è  Batch size {BATCH_SIZE} –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –¥–ª—è CPU")
        if max_workers > 20:
            logger.warning(f"‚ö†Ô∏è  Workers {max_workers} –º–æ–≥—É—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å —Å–∏—Å—Ç–µ–º—É")

    # –ü—É—Ç—å –∫ NPZ –¥–∞–Ω–Ω—ã–º
    NPZ_DIR = args.npz_dir

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if not os.path.exists(NPZ_DIR) or args.test_mode:
        if args.test_mode:
            logger.info("üß™ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ NPZ —Ñ–∞–π–ª—ã...")
        else:
            logger.info("üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ NPZ —Ñ–∞–π–ª—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")

        test_dir = "test_npz_data"
        os.makedirs(test_dir, exist_ok=True)

        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω—å—à–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ CPU
        num_test_files = 3 if args.test_mode else 5
        create_test_npz_files(test_dir, num_files=num_test_files)
        NPZ_DIR = test_dir
        logger.info(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –≤ {NPZ_DIR}")

    # –°–æ–∑–¥–∞–Ω–∏–µ data loaders —Å CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    try:
        train_loader, val_loader, preprocessor, analysis = create_amsr2_data_loaders(
            npz_dir=NPZ_DIR,
            batch_size=BATCH_SIZE,
            val_split=args.val_split,
            num_workers=max_workers,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers
            filter_orbit_type=args.orbit_filter
        )

        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   Total swaths: {analysis.get('total_swaths_found', 0)}")
        logger.info(f"   Train swaths: {len(train_loader.dataset)}")
        logger.info(f"   Val swaths: {len(val_loader.dataset)}")
        logger.info(f"   Orbit distribution: {analysis.get('orbit_distribution', {})}")
        logger.info(f"   Target size: {preprocessor.target_height}x{preprocessor.target_width}")

        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è CPU
        if DEVICE.type == 'cpu':
            total_swaths = len(train_loader.dataset)
            estimated_time_hours = (total_swaths * NUM_EPOCHS) / (max_cores * 50)  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            logger.info(f"‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {estimated_time_hours:.1f} —á–∞—Å–æ–≤")
            if estimated_time_hours > 100:
                logger.warning(
                    "‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ epochs –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è data loaders: {e}")
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = UNetResNetSuperResolution(
        in_channels=1,
        out_channels=1,
        scale_factor=SCALE_FACTOR
    )

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_mb = total_params * 4 / 1024 / 1024  # float32

    logger.info(f"üß† –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
    logger.info(f"   –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,}")
    logger.info(f"   –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,}")
    logger.info(f"   –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {model_size_mb:.1f} MB")
    logger.info(f"   Scale factor: {SCALE_FACTOR}x")

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = AMSR2SuperResolutionTrainer(
        model=model,
        device=DEVICE,
        learning_rate=LEARNING_RATE
    )

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ä—Ç–µ –æ–±—É—á–µ–Ω–∏—è
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:")
    logger.info(f"   –≠–ø–æ—Ö–∏: {NUM_EPOCHS}")
    logger.info(f"   Batch size: {BATCH_SIZE}")
    logger.info(f"   Learning rate: {LEARNING_RATE}")
    logger.info(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {args.save_path}")

    # –û–±—É—á–µ–Ω–∏–µ
    training_start_time = time.time()
    training_results = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=NUM_EPOCHS,
        save_path=args.save_path
    )
    training_end_time = time.time()

    total_training_time = training_end_time - training_start_time
    logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_training_time / 3600:.2f} —á–∞—Å–æ–≤")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.plot_training_history("amsr2_training_history.png")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    test_metrics = test_model_inference(
        model=model,
        test_loader=val_loader,
        device=DEVICE,
        save_examples=True,
        preprocessor=preprocessor
    )
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=NUM_EPOCHS,
        save_path="best_amsr2_model.pth"
    )

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.plot_training_history("amsr2_training_history.png")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    final_stats = {
        'training_results': training_results,
        'test_metrics': test_metrics,
        'data_analysis': analysis,
        'model_info': {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'scale_factor': SCALE_FACTOR,
            'target_size': f"{preprocessor.target_height}x{preprocessor.target_width}"
        },
        'training_config': {
            'batch_size': BATCH_SIZE,
            'num_epochs': NUM_EPOCHS,
            'learning_rate': LEARNING_RATE,
            'orbit_filter': args.orbit_filter,
            'device': str(DEVICE),
            'total_training_time_hours': total_training_time / 3600
        },
        'system_info': {
            'cpu_cores_total': psutil.cpu_count(),
            'cpu_cores_used': max_cores,
            'cpu_cores_free': psutil.cpu_count() - max_cores,
            'dataloader_workers': max_workers,
            'memory_total_gb': psutil.virtual_memory().total / 1024 ** 3,
            'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}",
            'pytorch_version': torch.__version__
        }
    }

    with open('amsr2_training_summary.json', 'w') as f:
        json.dump(final_stats, f, indent=2, default=str)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    logger.info("üìÅ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    logger.info(f"   - {args.save_path} (–≤–µ—Å–∞ –º–æ–¥–µ–ª–∏)")
    logger.info("   - amsr2_training_history.png (–≥—Ä–∞—Ñ–∏–∫–∏)")
    logger.info("   - amsr2_training_summary.json (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)")
    logger.info("   - amsr2_example_*.png (–ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if DEVICE.type == 'cpu':
        final_psnr = test_metrics.get('PSNR', 0)
        if final_psnr > 30:
            logger.info("‚úÖ –û—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! PSNR > 30 dB")
        elif final_psnr > 25:
            logger.info("‚úÖ –•–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! PSNR > 25 dB")
        else:
            logger.info("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            logger.info("   - –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
            logger.info("   - –£–º–µ–Ω—å—à–∏—Ç—å learning rate")
            logger.info("   - –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–∞—Ö
    logger.info("\nüìù –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    logger.info("   1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ amsr2_training_history.png")
    logger.info("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (amsr2_example_*.png)")
    logger.info("   3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è inference –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    logger.info(f"      process_new_npz_file('{args.save_path}', 'new_data.npz', 'result.npz')")


if __name__ == "__main__":
    main()